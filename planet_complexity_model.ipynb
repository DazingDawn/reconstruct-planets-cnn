{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries Used"
      ],
      "metadata": {
        "id": "ZwIQj4_-3Ij9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as r\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Flatten, AveragePooling1D, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "c8Y7lhn-ndU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "3qWDBYMctOc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(N, epsilon, S):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    csv_data = [pd.read_csv(f\"/Users/Jerry/Downloads/Caltech Research Datasets/FullDataset/W{i+1}.csv\").values for i in range(5)]\n",
        "\n",
        "    for _ in range(S):\n",
        "        file_idx = r.randint(0, 4)\n",
        "        file_data = csv_data[file_idx]\n",
        "\n",
        "        if N > len(file_data):\n",
        "            raise Exception(\"N is greater than the length of the file data.\")\n",
        "\n",
        "        start_idx = r.randint(0, len(file_data) - N)\n",
        "\n",
        "        section = file_data[start_idx:start_idx + N]\n",
        "\n",
        "        noise = np.random.normal(0, epsilon, section.shape)\n",
        "        section = section + noise\n",
        "\n",
        "        data.append(section)\n",
        "        labels.append(file_idx + 1)\n",
        "\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    num_classes = 5\n",
        "    labels_one_hot = np.eye(num_classes)[labels - 1]\n",
        "\n",
        "    return data, labels_one_hot\n",
        "\n",
        "sections, labels_one_hot = preprocess_data(2000, 0.01, 200)"
      ],
      "metadata": {
        "id": "-_92rdc_NexQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model Training Zone"
      ],
      "metadata": {
        "id": "6Mt8O7Mb3RTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (sections.shape[1], sections.shape[2])\n",
        "num_classes = 5\n",
        "# start changing network\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, activation='gelu', input_shape=input_shape),\n",
        "    AveragePooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dropout(0.7),\n",
        "    Dense(100, activation='softmax'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(sections, labels_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Fit the model with early stopping\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=100,  # Increased the number of epochs\n",
        "                    batch_size=16,  # Adjusted batch size\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[early_stopping],\n",
        "                    verbose=2)\n",
        "\n",
        "model.summary(), history.history"
      ],
      "metadata": {
        "id": "8XeX70t143xt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec0f5c1-a44a-4bf0-cbac-ef4bebe34140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10/10 - 1s - loss: 1.6187 - accuracy: 0.1937 - val_loss: 1.5918 - val_accuracy: 0.5250 - 1s/epoch - 138ms/step\n",
            "Epoch 2/100\n",
            "10/10 - 1s - loss: 1.5736 - accuracy: 0.3187 - val_loss: 1.5296 - val_accuracy: 0.3500 - 705ms/epoch - 70ms/step\n",
            "Epoch 3/100\n",
            "10/10 - 1s - loss: 1.5085 - accuracy: 0.3938 - val_loss: 1.4966 - val_accuracy: 0.2750 - 688ms/epoch - 69ms/step\n",
            "Epoch 4/100\n",
            "10/10 - 1s - loss: 1.4827 - accuracy: 0.3688 - val_loss: 1.4777 - val_accuracy: 0.3500 - 689ms/epoch - 69ms/step\n",
            "Epoch 5/100\n",
            "10/10 - 1s - loss: 1.4675 - accuracy: 0.3938 - val_loss: 1.4562 - val_accuracy: 0.5000 - 774ms/epoch - 77ms/step\n",
            "Epoch 6/100\n",
            "10/10 - 1s - loss: 1.4436 - accuracy: 0.4875 - val_loss: 1.4260 - val_accuracy: 0.4750 - 831ms/epoch - 83ms/step\n",
            "Epoch 7/100\n",
            "10/10 - 1s - loss: 1.4161 - accuracy: 0.5938 - val_loss: 1.3954 - val_accuracy: 0.5000 - 841ms/epoch - 84ms/step\n",
            "Epoch 8/100\n",
            "10/10 - 1s - loss: 1.3944 - accuracy: 0.5250 - val_loss: 1.3768 - val_accuracy: 0.5500 - 833ms/epoch - 83ms/step\n",
            "Epoch 9/100\n",
            "10/10 - 1s - loss: 1.3742 - accuracy: 0.5813 - val_loss: 1.3639 - val_accuracy: 0.8000 - 827ms/epoch - 83ms/step\n",
            "Epoch 10/100\n",
            "10/10 - 1s - loss: 1.3586 - accuracy: 0.6062 - val_loss: 1.3405 - val_accuracy: 0.5250 - 838ms/epoch - 84ms/step\n",
            "Epoch 11/100\n",
            "10/10 - 1s - loss: 1.3403 - accuracy: 0.5688 - val_loss: 1.3266 - val_accuracy: 0.5250 - 834ms/epoch - 83ms/step\n",
            "Epoch 12/100\n",
            "10/10 - 1s - loss: 1.3239 - accuracy: 0.5688 - val_loss: 1.3123 - val_accuracy: 0.5250 - 840ms/epoch - 84ms/step\n",
            "Epoch 13/100\n",
            "10/10 - 1s - loss: 1.3082 - accuracy: 0.6875 - val_loss: 1.2999 - val_accuracy: 0.7250 - 828ms/epoch - 83ms/step\n",
            "Epoch 14/100\n",
            "10/10 - 1s - loss: 1.2942 - accuracy: 0.7688 - val_loss: 1.2870 - val_accuracy: 0.7250 - 726ms/epoch - 73ms/step\n",
            "Epoch 15/100\n",
            "10/10 - 1s - loss: 1.2806 - accuracy: 0.7688 - val_loss: 1.2752 - val_accuracy: 0.7250 - 784ms/epoch - 78ms/step\n",
            "Epoch 16/100\n",
            "10/10 - 1s - loss: 1.2677 - accuracy: 0.7688 - val_loss: 1.2634 - val_accuracy: 0.7250 - 736ms/epoch - 74ms/step\n",
            "Epoch 17/100\n",
            "10/10 - 1s - loss: 1.2550 - accuracy: 0.7688 - val_loss: 1.2523 - val_accuracy: 0.7250 - 839ms/epoch - 84ms/step\n",
            "Epoch 18/100\n",
            "10/10 - 1s - loss: 1.2429 - accuracy: 0.7688 - val_loss: 1.2414 - val_accuracy: 0.7250 - 832ms/epoch - 83ms/step\n",
            "Epoch 19/100\n",
            "10/10 - 1s - loss: 1.2310 - accuracy: 0.7688 - val_loss: 1.2308 - val_accuracy: 0.7250 - 786ms/epoch - 79ms/step\n",
            "Epoch 20/100\n",
            "10/10 - 1s - loss: 1.2192 - accuracy: 0.7688 - val_loss: 1.2205 - val_accuracy: 0.7250 - 778ms/epoch - 78ms/step\n",
            "Epoch 21/100\n",
            "10/10 - 1s - loss: 1.2078 - accuracy: 0.7688 - val_loss: 1.2102 - val_accuracy: 0.7250 - 851ms/epoch - 85ms/step\n",
            "Epoch 22/100\n",
            "10/10 - 1s - loss: 1.1965 - accuracy: 0.7688 - val_loss: 1.2002 - val_accuracy: 0.7250 - 804ms/epoch - 80ms/step\n",
            "Epoch 23/100\n",
            "10/10 - 1s - loss: 1.1857 - accuracy: 0.7688 - val_loss: 1.1906 - val_accuracy: 0.7250 - 695ms/epoch - 70ms/step\n",
            "Epoch 24/100\n",
            "10/10 - 1s - loss: 1.1748 - accuracy: 0.8313 - val_loss: 1.1810 - val_accuracy: 0.7000 - 728ms/epoch - 73ms/step\n",
            "Epoch 25/100\n",
            "10/10 - 1s - loss: 1.1642 - accuracy: 0.8062 - val_loss: 1.1715 - val_accuracy: 0.7000 - 707ms/epoch - 71ms/step\n",
            "Epoch 26/100\n",
            "10/10 - 1s - loss: 1.1538 - accuracy: 0.8062 - val_loss: 1.1624 - val_accuracy: 0.7000 - 800ms/epoch - 80ms/step\n",
            "Epoch 27/100\n",
            "10/10 - 1s - loss: 1.1434 - accuracy: 0.8062 - val_loss: 1.1534 - val_accuracy: 0.7000 - 729ms/epoch - 73ms/step\n",
            "Epoch 28/100\n",
            "10/10 - 1s - loss: 1.1335 - accuracy: 0.8062 - val_loss: 1.1444 - val_accuracy: 0.7000 - 764ms/epoch - 76ms/step\n",
            "Epoch 29/100\n",
            "10/10 - 1s - loss: 1.1235 - accuracy: 0.8062 - val_loss: 1.1358 - val_accuracy: 0.7000 - 783ms/epoch - 78ms/step\n",
            "Epoch 30/100\n",
            "10/10 - 1s - loss: 1.1137 - accuracy: 0.8062 - val_loss: 1.1272 - val_accuracy: 0.7000 - 752ms/epoch - 75ms/step\n",
            "Epoch 31/100\n",
            "10/10 - 1s - loss: 1.1043 - accuracy: 0.8062 - val_loss: 1.1189 - val_accuracy: 0.7000 - 736ms/epoch - 74ms/step\n",
            "Epoch 32/100\n",
            "10/10 - 1s - loss: 1.0947 - accuracy: 0.8062 - val_loss: 1.1105 - val_accuracy: 0.7000 - 777ms/epoch - 78ms/step\n",
            "Epoch 33/100\n",
            "10/10 - 1s - loss: 1.0852 - accuracy: 0.8062 - val_loss: 1.1021 - val_accuracy: 0.7000 - 735ms/epoch - 73ms/step\n",
            "Epoch 34/100\n",
            "10/10 - 1s - loss: 1.0761 - accuracy: 0.8062 - val_loss: 1.0940 - val_accuracy: 0.7000 - 799ms/epoch - 80ms/step\n",
            "Epoch 35/100\n",
            "10/10 - 1s - loss: 1.0671 - accuracy: 0.8062 - val_loss: 1.0860 - val_accuracy: 0.7000 - 752ms/epoch - 75ms/step\n",
            "Epoch 36/100\n",
            "10/10 - 1s - loss: 1.0580 - accuracy: 0.8062 - val_loss: 1.0781 - val_accuracy: 0.7000 - 724ms/epoch - 72ms/step\n",
            "Epoch 37/100\n",
            "10/10 - 1s - loss: 1.0491 - accuracy: 0.8062 - val_loss: 1.0703 - val_accuracy: 0.7000 - 729ms/epoch - 73ms/step\n",
            "Epoch 38/100\n",
            "10/10 - 1s - loss: 1.0405 - accuracy: 0.8062 - val_loss: 1.0626 - val_accuracy: 0.7000 - 722ms/epoch - 72ms/step\n",
            "Epoch 39/100\n",
            "10/10 - 1s - loss: 1.0320 - accuracy: 0.8062 - val_loss: 1.0551 - val_accuracy: 0.7000 - 716ms/epoch - 72ms/step\n",
            "Epoch 40/100\n",
            "10/10 - 1s - loss: 1.0234 - accuracy: 0.8062 - val_loss: 1.0476 - val_accuracy: 0.7000 - 753ms/epoch - 75ms/step\n",
            "Epoch 41/100\n",
            "10/10 - 1s - loss: 1.0150 - accuracy: 0.8062 - val_loss: 1.0403 - val_accuracy: 0.7000 - 778ms/epoch - 78ms/step\n",
            "Epoch 42/100\n",
            "10/10 - 1s - loss: 1.0066 - accuracy: 0.8062 - val_loss: 1.0330 - val_accuracy: 0.7000 - 714ms/epoch - 71ms/step\n",
            "Epoch 43/100\n",
            "10/10 - 1s - loss: 0.9986 - accuracy: 0.8062 - val_loss: 1.0259 - val_accuracy: 0.7000 - 730ms/epoch - 73ms/step\n",
            "Epoch 44/100\n",
            "10/10 - 1s - loss: 0.9905 - accuracy: 0.8062 - val_loss: 1.0190 - val_accuracy: 0.7000 - 739ms/epoch - 74ms/step\n",
            "Epoch 45/100\n",
            "10/10 - 1s - loss: 0.9827 - accuracy: 0.8062 - val_loss: 1.0121 - val_accuracy: 0.7000 - 694ms/epoch - 69ms/step\n",
            "Epoch 46/100\n",
            "10/10 - 1s - loss: 0.9747 - accuracy: 0.8062 - val_loss: 1.0052 - val_accuracy: 0.7000 - 689ms/epoch - 69ms/step\n",
            "Epoch 47/100\n",
            "10/10 - 1s - loss: 0.9669 - accuracy: 0.8062 - val_loss: 0.9982 - val_accuracy: 0.7000 - 698ms/epoch - 70ms/step\n",
            "Epoch 48/100\n",
            "10/10 - 1s - loss: 0.9595 - accuracy: 0.8062 - val_loss: 0.9916 - val_accuracy: 0.7000 - 764ms/epoch - 76ms/step\n",
            "Epoch 49/100\n",
            "10/10 - 1s - loss: 0.9519 - accuracy: 0.8062 - val_loss: 0.9850 - val_accuracy: 0.7000 - 719ms/epoch - 72ms/step\n",
            "Epoch 50/100\n",
            "10/10 - 1s - loss: 0.9444 - accuracy: 0.8062 - val_loss: 0.9786 - val_accuracy: 0.7000 - 738ms/epoch - 74ms/step\n",
            "Epoch 51/100\n",
            "10/10 - 1s - loss: 0.9370 - accuracy: 0.8062 - val_loss: 0.9722 - val_accuracy: 0.7000 - 740ms/epoch - 74ms/step\n",
            "Epoch 52/100\n",
            "10/10 - 1s - loss: 0.9297 - accuracy: 0.8062 - val_loss: 0.9658 - val_accuracy: 0.7000 - 728ms/epoch - 73ms/step\n",
            "Epoch 53/100\n",
            "10/10 - 1s - loss: 0.9226 - accuracy: 0.8062 - val_loss: 0.9596 - val_accuracy: 0.7000 - 726ms/epoch - 73ms/step\n",
            "Epoch 54/100\n",
            "10/10 - 1s - loss: 0.9156 - accuracy: 0.8062 - val_loss: 0.9536 - val_accuracy: 0.7000 - 719ms/epoch - 72ms/step\n",
            "Epoch 55/100\n",
            "10/10 - 1s - loss: 0.9086 - accuracy: 0.8062 - val_loss: 0.9474 - val_accuracy: 0.7000 - 744ms/epoch - 74ms/step\n",
            "Epoch 56/100\n",
            "10/10 - 1s - loss: 0.9018 - accuracy: 0.8062 - val_loss: 0.9416 - val_accuracy: 0.7000 - 726ms/epoch - 73ms/step\n",
            "Epoch 57/100\n",
            "10/10 - 1s - loss: 0.8949 - accuracy: 0.8062 - val_loss: 0.9355 - val_accuracy: 0.7000 - 722ms/epoch - 72ms/step\n",
            "Epoch 58/100\n",
            "10/10 - 1s - loss: 0.8883 - accuracy: 0.8062 - val_loss: 0.9297 - val_accuracy: 0.7000 - 721ms/epoch - 72ms/step\n",
            "Epoch 59/100\n",
            "10/10 - 1s - loss: 0.8816 - accuracy: 0.8062 - val_loss: 0.9240 - val_accuracy: 0.7000 - 787ms/epoch - 79ms/step\n",
            "Epoch 60/100\n",
            "10/10 - 1s - loss: 0.8751 - accuracy: 0.8062 - val_loss: 0.9183 - val_accuracy: 0.7000 - 821ms/epoch - 82ms/step\n",
            "Epoch 61/100\n",
            "10/10 - 1s - loss: 0.8686 - accuracy: 0.8062 - val_loss: 0.9127 - val_accuracy: 0.7000 - 825ms/epoch - 83ms/step\n",
            "Epoch 62/100\n",
            "10/10 - 1s - loss: 0.8622 - accuracy: 0.8062 - val_loss: 0.9070 - val_accuracy: 0.7000 - 868ms/epoch - 87ms/step\n",
            "Epoch 63/100\n",
            "10/10 - 1s - loss: 0.8560 - accuracy: 0.8062 - val_loss: 0.9016 - val_accuracy: 0.7000 - 728ms/epoch - 73ms/step\n",
            "Epoch 64/100\n",
            "10/10 - 1s - loss: 0.8497 - accuracy: 0.8062 - val_loss: 0.8963 - val_accuracy: 0.7000 - 741ms/epoch - 74ms/step\n",
            "Epoch 65/100\n",
            "10/10 - 1s - loss: 0.8436 - accuracy: 0.8062 - val_loss: 0.8909 - val_accuracy: 0.7000 - 682ms/epoch - 68ms/step\n",
            "Epoch 66/100\n",
            "10/10 - 1s - loss: 0.8376 - accuracy: 0.8062 - val_loss: 0.8856 - val_accuracy: 0.7000 - 766ms/epoch - 77ms/step\n",
            "Epoch 67/100\n",
            "10/10 - 1s - loss: 0.8317 - accuracy: 0.8062 - val_loss: 0.8805 - val_accuracy: 0.7000 - 745ms/epoch - 74ms/step\n",
            "Epoch 68/100\n",
            "10/10 - 1s - loss: 0.8256 - accuracy: 0.8062 - val_loss: 0.8754 - val_accuracy: 0.7000 - 690ms/epoch - 69ms/step\n",
            "Epoch 69/100\n",
            "10/10 - 1s - loss: 0.8199 - accuracy: 0.8062 - val_loss: 0.8703 - val_accuracy: 0.7000 - 663ms/epoch - 66ms/step\n",
            "Epoch 70/100\n",
            "10/10 - 1s - loss: 0.8142 - accuracy: 0.8062 - val_loss: 0.8654 - val_accuracy: 0.7000 - 729ms/epoch - 73ms/step\n",
            "Epoch 71/100\n",
            "10/10 - 1s - loss: 0.8085 - accuracy: 0.8062 - val_loss: 0.8604 - val_accuracy: 0.7000 - 686ms/epoch - 69ms/step\n",
            "Epoch 72/100\n",
            "10/10 - 1s - loss: 0.8029 - accuracy: 0.8062 - val_loss: 0.8556 - val_accuracy: 0.7000 - 697ms/epoch - 70ms/step\n",
            "Epoch 73/100\n",
            "10/10 - 1s - loss: 0.7973 - accuracy: 0.8062 - val_loss: 0.8507 - val_accuracy: 0.7000 - 728ms/epoch - 73ms/step\n",
            "Epoch 74/100\n",
            "10/10 - 1s - loss: 0.7920 - accuracy: 0.8062 - val_loss: 0.8460 - val_accuracy: 0.7000 - 698ms/epoch - 70ms/step\n",
            "Epoch 75/100\n",
            "10/10 - 1s - loss: 0.7865 - accuracy: 0.8062 - val_loss: 0.8413 - val_accuracy: 0.7000 - 710ms/epoch - 71ms/step\n",
            "Epoch 76/100\n",
            "10/10 - 1s - loss: 0.7812 - accuracy: 0.8062 - val_loss: 0.8366 - val_accuracy: 0.7000 - 778ms/epoch - 78ms/step\n",
            "Epoch 77/100\n",
            "10/10 - 1s - loss: 0.7759 - accuracy: 0.8062 - val_loss: 0.8321 - val_accuracy: 0.7000 - 711ms/epoch - 71ms/step\n",
            "Epoch 78/100\n",
            "10/10 - 1s - loss: 0.7707 - accuracy: 0.8062 - val_loss: 0.8276 - val_accuracy: 0.7000 - 769ms/epoch - 77ms/step\n",
            "Epoch 79/100\n",
            "10/10 - 1s - loss: 0.7656 - accuracy: 0.8062 - val_loss: 0.8231 - val_accuracy: 0.7000 - 722ms/epoch - 72ms/step\n",
            "Epoch 80/100\n",
            "10/10 - 1s - loss: 0.7606 - accuracy: 0.8062 - val_loss: 0.8187 - val_accuracy: 0.7000 - 692ms/epoch - 69ms/step\n",
            "Epoch 81/100\n",
            "10/10 - 1s - loss: 0.7556 - accuracy: 0.8062 - val_loss: 0.8144 - val_accuracy: 0.7000 - 713ms/epoch - 71ms/step\n",
            "Epoch 82/100\n",
            "10/10 - 1s - loss: 0.7507 - accuracy: 0.8062 - val_loss: 0.8101 - val_accuracy: 0.7000 - 700ms/epoch - 70ms/step\n",
            "Epoch 83/100\n",
            "10/10 - 1s - loss: 0.7458 - accuracy: 0.8062 - val_loss: 0.8059 - val_accuracy: 0.7000 - 780ms/epoch - 78ms/step\n",
            "Epoch 84/100\n",
            "10/10 - 1s - loss: 0.7411 - accuracy: 0.8062 - val_loss: 0.8017 - val_accuracy: 0.7000 - 747ms/epoch - 75ms/step\n",
            "Epoch 85/100\n",
            "10/10 - 1s - loss: 0.7363 - accuracy: 0.8062 - val_loss: 0.7975 - val_accuracy: 0.7000 - 720ms/epoch - 72ms/step\n",
            "Epoch 86/100\n",
            "10/10 - 1s - loss: 0.7316 - accuracy: 0.8062 - val_loss: 0.7936 - val_accuracy: 0.7000 - 770ms/epoch - 77ms/step\n",
            "Epoch 87/100\n",
            "10/10 - 1s - loss: 0.7270 - accuracy: 0.8062 - val_loss: 0.7896 - val_accuracy: 0.7000 - 787ms/epoch - 79ms/step\n",
            "Epoch 88/100\n",
            "10/10 - 1s - loss: 0.7225 - accuracy: 0.8062 - val_loss: 0.7857 - val_accuracy: 0.7000 - 798ms/epoch - 80ms/step\n",
            "Epoch 89/100\n",
            "10/10 - 1s - loss: 0.7180 - accuracy: 0.8062 - val_loss: 0.7819 - val_accuracy: 0.7000 - 819ms/epoch - 82ms/step\n",
            "Epoch 90/100\n",
            "10/10 - 1s - loss: 0.7135 - accuracy: 0.8062 - val_loss: 0.7778 - val_accuracy: 0.7000 - 780ms/epoch - 78ms/step\n",
            "Epoch 91/100\n",
            "10/10 - 1s - loss: 0.7092 - accuracy: 0.8062 - val_loss: 0.7739 - val_accuracy: 0.7000 - 745ms/epoch - 75ms/step\n",
            "Epoch 92/100\n",
            "10/10 - 1s - loss: 0.7048 - accuracy: 0.8062 - val_loss: 0.7703 - val_accuracy: 0.7000 - 721ms/epoch - 72ms/step\n",
            "Epoch 93/100\n",
            "10/10 - 1s - loss: 0.7006 - accuracy: 0.8062 - val_loss: 0.7666 - val_accuracy: 0.7000 - 706ms/epoch - 71ms/step\n",
            "Epoch 94/100\n",
            "10/10 - 1s - loss: 0.6964 - accuracy: 0.8062 - val_loss: 0.7630 - val_accuracy: 0.7000 - 705ms/epoch - 70ms/step\n",
            "Epoch 95/100\n",
            "10/10 - 1s - loss: 0.6922 - accuracy: 0.8062 - val_loss: 0.7594 - val_accuracy: 0.7000 - 706ms/epoch - 71ms/step\n",
            "Epoch 96/100\n",
            "10/10 - 1s - loss: 0.6881 - accuracy: 0.8062 - val_loss: 0.7559 - val_accuracy: 0.7000 - 685ms/epoch - 69ms/step\n",
            "Epoch 97/100\n",
            "10/10 - 1s - loss: 0.6841 - accuracy: 0.8062 - val_loss: 0.7524 - val_accuracy: 0.7000 - 718ms/epoch - 72ms/step\n",
            "Epoch 98/100\n",
            "10/10 - 1s - loss: 0.6801 - accuracy: 0.8062 - val_loss: 0.7488 - val_accuracy: 0.7000 - 701ms/epoch - 70ms/step\n",
            "Epoch 99/100\n",
            "10/10 - 1s - loss: 0.6762 - accuracy: 0.8062 - val_loss: 0.7455 - val_accuracy: 0.7000 - 685ms/epoch - 68ms/step\n",
            "Epoch 100/100\n",
            "10/10 - 1s - loss: 0.6722 - accuracy: 0.8062 - val_loss: 0.7420 - val_accuracy: 0.7000 - 717ms/epoch - 72ms/step\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_3 (Conv1D)           (None, 1998, 64)          1024      \n",
            "                                                                 \n",
            " average_pooling1d_3 (Avera  (None, 999, 64)           0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 63936)             0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 63936)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 100)               6393700   \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 5)                 505       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6395229 (24.40 MB)\n",
            "Trainable params: 6395229 (24.40 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None,\n",
              " {'loss': [1.6187198162078857,\n",
              "   1.5735626220703125,\n",
              "   1.5085471868515015,\n",
              "   1.4826533794403076,\n",
              "   1.467527985572815,\n",
              "   1.4436274766921997,\n",
              "   1.416090726852417,\n",
              "   1.3944379091262817,\n",
              "   1.3741819858551025,\n",
              "   1.358614206314087,\n",
              "   1.3403493165969849,\n",
              "   1.32387375831604,\n",
              "   1.3082150220870972,\n",
              "   1.2942074537277222,\n",
              "   1.2805984020233154,\n",
              "   1.2677475214004517,\n",
              "   1.2550262212753296,\n",
              "   1.2429404258728027,\n",
              "   1.2309799194335938,\n",
              "   1.2191592454910278,\n",
              "   1.2077691555023193,\n",
              "   1.1965116262435913,\n",
              "   1.1856579780578613,\n",
              "   1.174760103225708,\n",
              "   1.164224624633789,\n",
              "   1.1537551879882812,\n",
              "   1.143431305885315,\n",
              "   1.1334999799728394,\n",
              "   1.1235425472259521,\n",
              "   1.1136882305145264,\n",
              "   1.1042567491531372,\n",
              "   1.0947452783584595,\n",
              "   1.0852348804473877,\n",
              "   1.0761325359344482,\n",
              "   1.0670894384384155,\n",
              "   1.0579875707626343,\n",
              "   1.0491329431533813,\n",
              "   1.0404807329177856,\n",
              "   1.0319738388061523,\n",
              "   1.0233523845672607,\n",
              "   1.0149978399276733,\n",
              "   1.0066204071044922,\n",
              "   0.9986039996147156,\n",
              "   0.9904535412788391,\n",
              "   0.9826637506484985,\n",
              "   0.974696934223175,\n",
              "   0.9669414758682251,\n",
              "   0.9594962000846863,\n",
              "   0.951898455619812,\n",
              "   0.9444195628166199,\n",
              "   0.9369671940803528,\n",
              "   0.92973393201828,\n",
              "   0.9226455688476562,\n",
              "   0.9155569076538086,\n",
              "   0.9086171984672546,\n",
              "   0.9017993211746216,\n",
              "   0.8949396014213562,\n",
              "   0.8882856369018555,\n",
              "   0.8815760612487793,\n",
              "   0.8750594854354858,\n",
              "   0.8686315417289734,\n",
              "   0.8621798753738403,\n",
              "   0.8560066223144531,\n",
              "   0.8497312664985657,\n",
              "   0.8435619473457336,\n",
              "   0.8375652432441711,\n",
              "   0.8316863179206848,\n",
              "   0.8256394267082214,\n",
              "   0.819902241230011,\n",
              "   0.8141992688179016,\n",
              "   0.8084751963615417,\n",
              "   0.8028565645217896,\n",
              "   0.7973078489303589,\n",
              "   0.7919945120811462,\n",
              "   0.7864670753479004,\n",
              "   0.7811785936355591,\n",
              "   0.7758829593658447,\n",
              "   0.770712673664093,\n",
              "   0.7656163573265076,\n",
              "   0.7605889439582825,\n",
              "   0.7555667161941528,\n",
              "   0.750684916973114,\n",
              "   0.745801568031311,\n",
              "   0.7410701513290405,\n",
              "   0.7363356351852417,\n",
              "   0.7315779328346252,\n",
              "   0.7269874811172485,\n",
              "   0.7225421667098999,\n",
              "   0.7179839015007019,\n",
              "   0.7135175466537476,\n",
              "   0.7091974020004272,\n",
              "   0.7048064470291138,\n",
              "   0.7005928158760071,\n",
              "   0.6963835954666138,\n",
              "   0.6922207474708557,\n",
              "   0.6881189942359924,\n",
              "   0.6840947866439819,\n",
              "   0.6800915598869324,\n",
              "   0.6762017011642456,\n",
              "   0.6721745133399963],\n",
              "  'accuracy': [0.19374999403953552,\n",
              "   0.3187499940395355,\n",
              "   0.39375001192092896,\n",
              "   0.3687500059604645,\n",
              "   0.39375001192092896,\n",
              "   0.48750001192092896,\n",
              "   0.59375,\n",
              "   0.5249999761581421,\n",
              "   0.581250011920929,\n",
              "   0.606249988079071,\n",
              "   0.5687500238418579,\n",
              "   0.5687500238418579,\n",
              "   0.6875,\n",
              "   0.768750011920929,\n",
              "   0.768750011920929,\n",
              "   0.768750011920929,\n",
              "   0.768750011920929,\n",
              "   0.768750011920929,\n",
              "   0.768750011920929,\n",
              "   0.768750011920929,\n",
              "   0.768750011920929,\n",
              "   0.768750011920929,\n",
              "   0.768750011920929,\n",
              "   0.831250011920929,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421,\n",
              "   0.8062499761581421],\n",
              "  'val_loss': [1.591844916343689,\n",
              "   1.5295684337615967,\n",
              "   1.4966020584106445,\n",
              "   1.4776808023452759,\n",
              "   1.4561687707901,\n",
              "   1.425963044166565,\n",
              "   1.3954452276229858,\n",
              "   1.3767694234848022,\n",
              "   1.36385178565979,\n",
              "   1.3405330181121826,\n",
              "   1.326637864112854,\n",
              "   1.312333106994629,\n",
              "   1.2998626232147217,\n",
              "   1.2869828939437866,\n",
              "   1.2751901149749756,\n",
              "   1.2633699178695679,\n",
              "   1.2523458003997803,\n",
              "   1.241430640220642,\n",
              "   1.2308404445648193,\n",
              "   1.2204906940460205,\n",
              "   1.2102090120315552,\n",
              "   1.2002462148666382,\n",
              "   1.190618872642517,\n",
              "   1.180991768836975,\n",
              "   1.1715326309204102,\n",
              "   1.1623685359954834,\n",
              "   1.1533623933792114,\n",
              "   1.144394040107727,\n",
              "   1.135752558708191,\n",
              "   1.1272262334823608,\n",
              "   1.1188513040542603,\n",
              "   1.1104750633239746,\n",
              "   1.1021091938018799,\n",
              "   1.094020128250122,\n",
              "   1.085970163345337,\n",
              "   1.0781304836273193,\n",
              "   1.0703071355819702,\n",
              "   1.0625803470611572,\n",
              "   1.055083990097046,\n",
              "   1.047574758529663,\n",
              "   1.0403071641921997,\n",
              "   1.0330264568328857,\n",
              "   1.0259441137313843,\n",
              "   1.018996000289917,\n",
              "   1.012058973312378,\n",
              "   1.0051612854003906,\n",
              "   0.9982233047485352,\n",
              "   0.9915609359741211,\n",
              "   0.9850122332572937,\n",
              "   0.9785812497138977,\n",
              "   0.9721647500991821,\n",
              "   0.9657818675041199,\n",
              "   0.9596479535102844,\n",
              "   0.9536018371582031,\n",
              "   0.9474366307258606,\n",
              "   0.9415621757507324,\n",
              "   0.9355494379997253,\n",
              "   0.9297344088554382,\n",
              "   0.9240320920944214,\n",
              "   0.9182529449462891,\n",
              "   0.9127046465873718,\n",
              "   0.9070491790771484,\n",
              "   0.9016216397285461,\n",
              "   0.8962743878364563,\n",
              "   0.8908612132072449,\n",
              "   0.8856016993522644,\n",
              "   0.8805314302444458,\n",
              "   0.875424861907959,\n",
              "   0.8702661395072937,\n",
              "   0.8654182553291321,\n",
              "   0.8604364395141602,\n",
              "   0.8555788993835449,\n",
              "   0.8507243394851685,\n",
              "   0.8460062146186829,\n",
              "   0.8412668108940125,\n",
              "   0.8365852236747742,\n",
              "   0.8321104049682617,\n",
              "   0.8275642395019531,\n",
              "   0.823111355304718,\n",
              "   0.8187200427055359,\n",
              "   0.8144239187240601,\n",
              "   0.8101150393486023,\n",
              "   0.805855393409729,\n",
              "   0.8017467260360718,\n",
              "   0.797528862953186,\n",
              "   0.7936285734176636,\n",
              "   0.7895704507827759,\n",
              "   0.7857319712638855,\n",
              "   0.7818520069122314,\n",
              "   0.7778489589691162,\n",
              "   0.7739448547363281,\n",
              "   0.770283579826355,\n",
              "   0.7666023969650269,\n",
              "   0.7629988789558411,\n",
              "   0.7593798041343689,\n",
              "   0.7558954954147339,\n",
              "   0.7524480819702148,\n",
              "   0.7487994432449341,\n",
              "   0.7454512119293213,\n",
              "   0.7420481443405151],\n",
              "  'val_accuracy': [0.5249999761581421,\n",
              "   0.3499999940395355,\n",
              "   0.2750000059604645,\n",
              "   0.3499999940395355,\n",
              "   0.5,\n",
              "   0.4749999940395355,\n",
              "   0.5,\n",
              "   0.550000011920929,\n",
              "   0.800000011920929,\n",
              "   0.5249999761581421,\n",
              "   0.5249999761581421,\n",
              "   0.5249999761581421,\n",
              "   0.7250000238418579,\n",
              "   0.7250000238418579,\n",
              "   0.7250000238418579,\n",
              "   0.7250000238418579,\n",
              "   0.7250000238418579,\n",
              "   0.7250000238418579,\n",
              "   0.7250000238418579,\n",
              "   0.7250000238418579,\n",
              "   0.7250000238418579,\n",
              "   0.7250000238418579,\n",
              "   0.7250000238418579,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071,\n",
              "   0.699999988079071]})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting results"
      ],
      "metadata": {
        "id": "TOtzaz0c3UoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract loss and accuracy from the history object\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "epochs = range(1, len(train_loss) + 1)  # Assuming epochs index from 1\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss Plot\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
        "plt.plot(epochs, train_loss, label='Training Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
        "plt.plot(epochs, train_accuracy, label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rHNisRh5GgWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model"
      ],
      "metadata": {
        "id": "jSrVUkYp3Y8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/Users/Jerry/Downloads/Caltech Research Datasets/CNN_.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVQK46cBcF76",
        "outputId": "bb216f4b-8986-4403-ad18-3b24848add8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Jerry\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Pre-trained CNN model"
      ],
      "metadata": {
        "id": "AQoGwYTr3eU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = model"
      ],
      "metadata": {
        "id": "cUQ1XnY9szO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('/Users/Jerry/Downloads/Caltech Research Datasets/CNN.h5')"
      ],
      "metadata": {
        "id": "-S09HoCwdFbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Jupiter and Earth Classifications"
      ],
      "metadata": {
        "id": "JZFo9L4YfqCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_test_data():\n",
        "    E_443 = pd.read_csv('/Users/Jerry/Downloads/Caltech Research Datasets/ContinuousDataset/Earth_443.csv').values\n",
        "    E_551 = pd.read_csv('/Users/Jerry/Downloads/Caltech Research Datasets/ContinuousDataset/Earth_551.csv').values\n",
        "    E_764 = pd.read_csv('/Users/Jerry/Downloads/Caltech Research Datasets/ContinuousDataset/Earth_764.csv').values\n",
        "\n",
        "    J_443 = pd.read_csv('/Users/Jerry/Downloads/Caltech Research Datasets/ContinuousDataset/Jupiter_443.csv').values\n",
        "    J_551 = pd.read_csv('/Users/Jerry/Downloads/Caltech Research Datasets/ContinuousDataset/Jupiter_551.csv').values\n",
        "    J_764 = pd.read_csv('/Users/Jerry/Downloads/Caltech Research Datasets/ContinuousDataset/Jupiter_764.csv').values\n",
        "\n",
        "    Earth_data = np.concatenate((E_443, E_551, E_764), axis=1)\n",
        "    Jupiter_data = np.concatenate((J_443, J_551, J_764), axis=1)\n",
        "\n",
        "    return Earth_data, Jupiter_data\n",
        "\n",
        "Earth_data, Jupiter_data = preprocess_test_data()"
      ],
      "metadata": {
        "id": "Ox7tXAOO--qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earth = np.pad(Earth_data, ((0, 0), (0, 2)), mode='constant')\n",
        "jupiter = np.pad(Jupiter_data, ((0, 0), (0, 2)), mode='constant')"
      ],
      "metadata": {
        "id": "kzBKmPmQ--qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_data = np.pad(earth, ((0, 1637), (0, 0)), mode='constant')\n",
        "reshaped_data = padded_data.reshape((1, padded_data.shape[0], padded_data.shape[1]))\n",
        "predictionsEarth = new_model.predict(reshaped_data)\n",
        "print(predictionsEarth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8010663-fd84-4892-c61e-398479fae642",
        "id": "L0HJtxEs--qQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "[[1.0885820e-03 4.7938074e-03 4.5523360e-02 8.8090834e-05 9.4850612e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_data = np.pad(jupiter, ((0, 1637), (0, 0)), mode='constant')\n",
        "reshaped_data = padded_data.reshape((1, padded_data.shape[0], padded_data.shape[1]))\n",
        "predictionsJupiter = new_model.predict(reshaped_data)\n",
        "print(predictionsJupiter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c8d8ef1-100a-448d-994c-a7915096171f",
        "id": "zl0LI7UB--qQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "[[1.4835221e-02 1.0523765e-03 9.8349643e-01 6.0177456e-07 6.1538321e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classified Results for Earth and Jupiter"
      ],
      "metadata": {
        "id": "MPjDzLV03lb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earth_predicted_classes = np.argmax(predictionsEarth, axis=1)\n",
        "jupiter_predicted_classes = np.argmax(predictionsJupiter, axis=1)\n",
        "\n",
        "print(\"Predicted complexities for Earth dataset:\", earth_predicted_classes + 1)\n",
        "print(\"Predicted complexities for Jupiter dataset:\", jupiter_predicted_classes + 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7743bfa-9b45-49d5-fb98-8dad3ae29ab0",
        "id": "gNyugvLB--qQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted complexities for Earth dataset: [5]\n",
            "Predicted complexities for Jupiter dataset: [3]\n"
          ]
        }
      ]
    }
  ]
}